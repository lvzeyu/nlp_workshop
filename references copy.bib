---
---

@article{DIMAGGIO2013570,
title = {Exploiting affinities between topic modeling and the sociological perspective on culture: Application to newspaper coverage of U.S. government arts funding},
journal = {Poetics},
volume = {41},
number = {6},
pages = {570-606},
year = {2013},
note = {Topic Models and the Cultural Sciences},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2013.08.004},
author = {Paul DiMaggio and Manish Nag and David Blei},
keywords = {Topic models, Polysemy, Heteroglossia, Meaning, Content analysis, National Endowment for the Arts},
abstract = {Topic modeling provides a valuable method for identifying the linguistic contexts that surround social institutions or policy domains. This article uses Latent Dirichlet Allocation (LDA) to analyze how one such policy domain, government assistance to artists and arts organizations, was framed in almost 8000 articles. These comprised all articles that referred to government support for the arts in the U.S. published in five U.S. newspapers between 1986 and 1997—a period during which such assistance, once noncontroversial, became a focus of contention. We illustrate the strengths of topic modeling as a means of analyzing large text corpora, discuss the proper choice of models and interpretation of model results, describe means of validating topic-model solutions, and demonstrate the use of topic models in combination with other statistical tools to estimate differences between newspapers in the prevalence of different frames. Throughout, we emphasize affinities between the topic-modeling approach and such central concepts in the study of culture as framing, polysemy, heteroglossia, and the relationality of meaning.}
}

@article{
Garg2018,
author = {Nikhil Garg  and Londa Schiebinger  and Dan Jurafsky  and James Zou },
title = {Word embeddings quantify 100 years of gender and ethnic stereotypes},
journal = {Proceedings of the National Academy of Sciences},
volume = {115},
number = {16},
pages = {E3635-E3644},
year = {2018},
doi = {10.1073/pnas.1720347115},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1720347115},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1720347115},
abstract = {Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts—e.g., the women’s movement in the 1960s and Asian immigration into the United States—and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.}}

@article{Kozlowski2019,
author = {Austin C. Kozlowski and Matt Taddy and James A. Evans},
title ={The Geometry of Culture: Analyzing the Meanings of Class through Word Embeddings},
journal = {American Sociological Review},
volume = {84},
number = {5},
pages = {905-949},
year = {2019},
doi = {10.1177/0003122419877135},
,
    abstract = { We argue word embedding models are a useful tool for the study of culture using a historical analysis of shared understandings of social class as an empirical case. Word embeddings represent semantic relations between words as relationships between vectors in a high-dimensional space, specifying a relational model of meaning consistent with contemporary theories of culture. Dimensions induced by word differences (rich – poor) in these spaces correspond to dimensions of cultural meaning, and the projection of words onto these dimensions reflects widely shared associations, which we validate with surveys. Analyzing text from millions of books published over 100 years, we show that the markers of class continuously shifted amidst the economic transformations of the twentieth century, yet the basic cultural dimensions of class remained remarkably stable. The notable exception is education, which became tightly linked to affluence independent of its association with cultivated taste. }
}


@inproceedings{Devlin2019, 
year = {2019}, 
author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina}, 
title = {{\{BERT\}: Pre-training of Deep Bidirectional Transformers for Language Understanding}}, 
booktitle = {Proceedings of the 2019 Conference of the North \{A\}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, 
doi = {10.18653/v1/n19-1423}, 
url = {https://www.aclweb.org/anthology/N19-1423}, 
abstract = {{We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\textbackslash\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).}}, 
pages = {4171--4186}, 
series = {Proceedings of the 2019 Conference of the North}, 
publisher = {Association for Computational Linguistics}, 
address = {Minneapolis, Minnesota}, 
keywords = {}
}

@article{laurer_van,
 title={Less Annotating, More Classifying: Addressing the Data Scarcity Issue of Supervised Machine Learning with Deep Transfer Learning and BERT-NLI},
  DOI={10.1017/pan.2023.20},
  journal={Political Analysis},
  author={Laurer, Moritz and van Atteveldt, Wouter and Casas, Andreu and Welbers, Kasper},
   year={2023},
    pages={1–17}}